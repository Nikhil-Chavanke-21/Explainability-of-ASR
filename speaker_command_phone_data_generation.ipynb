{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speaker_command_phone_data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Chavanke-21/Explainability-of-ASR/blob/main/speaker_command_phone_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FSUvLRiEe1U"
      },
      "source": [
        "# installing the requirements\n",
        "!pip install datasets\n",
        "!pip install torchaudio\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5mV8HHPtI29"
      },
      "source": [
        "# importing the libraries\n",
        "import math\n",
        "import torch\n",
        "import pickle\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0cyggEPtLMZ"
      },
      "source": [
        "# Downloading and loading the Wav2Vec2Processor & Wav2Vec2Model Large Size\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NpcyfI36lUU"
      },
      "source": [
        "# For generating Speaker Classification data in to pickle file\n",
        "\n",
        "# only 25 speakers out of 600 are taken into for the classification task\n",
        "speakers={'FKSR0': 0, 'FMMH0': 1, 'MTPG0': 2, 'MTAT1': 3, 'MMDM1': 4, 'FPJF0': 5, 'FMML0': 6, 'MDLF0': 7, 'MRRK0': 8, 'MSES0': 9, 'MJES0': 10, 'FCJS0': 11, 'MCEM0': 12, 'FCAJ0': 13, 'FPMY0': 14, 'FKLC0': 15, 'FJSK0': 16, 'FNKL0': 17, 'FETB0': 18, 'FCLT0': 19, 'FLAS0': 20 , 'MJRF0': 21, 'MJDM0': 22, 'MDLD0': 23, 'MJDC0': 24}\n",
        "\n",
        "# reading wav file to get speech frames as a array\n",
        "def map_to_array(batch):\n",
        "  speech, _ = sf.read(batch[\"file\"])\n",
        "  batch[\"speech\"] = speech\n",
        "  return batch\n",
        "\n",
        "# getting train, test split of the dataset\n",
        "train_dataset=load_dataset(\"timit_asr\",split='train')\n",
        "test_dataset=load_dataset(\"timit_asr\",split='test')\n",
        "\n",
        "# array from wav file\n",
        "train_dataset = train_dataset.map(map_to_array)\n",
        "test_dataset = test_dataset.map(map_to_array)\n",
        "\n",
        "n=len(train_dataset)+len(test_dataset)\n",
        "permutation=np.random.permutation(n)\n",
        "\n",
        "# Opening files to pickle dump all attention layer representations for further classification\n",
        "input_file=open(\"x.txt\", \"wb\")\n",
        "target_file=open(\"y.txt\", \"wb\")\n",
        "print('total:',n)\n",
        "count=0\n",
        "y=[]\n",
        "\n",
        "# permutation for shuffling\n",
        "for i in permutation:\n",
        "  print('count',count)\n",
        "  count+=1\n",
        "  data=0\n",
        "  # mixing train and test data\n",
        "  if i<len(train_dataset):\n",
        "    data=train_dataset[int(i)]\n",
        "  else:\n",
        "    data=train_dataset[int(i)-len(train_dataset)]\n",
        "  \n",
        "  # taking only 25 speakers data\n",
        "  if data['speaker_id'] not in speakers:\n",
        "    continue\n",
        "\n",
        "  # preprocessing using Wav2Vec2Processor\n",
        "  x=processor(data[\"speech\"], return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "  # Generating data representations using Wav2Vec2Model\n",
        "  x=model.forward(x,output_hidden_states=True)\n",
        "  x=list(x.hidden_states)\n",
        "\n",
        "  # average pooling accross timeframes\n",
        "  for i, a in enumerate(x):\n",
        "    x[i]=torch.sum(a,(0,1))\n",
        "  # dumping all hidden activations\n",
        "  pickle.dump(x, input_file)\n",
        "  # getting speaker class index\n",
        "  y.append(speakers[data['speaker_id']])\n",
        "\n",
        "# dumping the class indices\n",
        "pickle.dump(y,target_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk5FUiaXErwZ"
      },
      "source": [
        "# For generating Speaker Classification data in to pickle file\n",
        "\n",
        "# only 25 speakers out of 600 are taken into for the classification task\n",
        "speakers={'FKSR0': 0, 'FMMH0': 1, 'MTPG0': 2, 'MTAT1': 3, 'MMDM1': 4, 'FPJF0': 5, 'FMML0': 6, 'MDLF0': 7, 'MRRK0': 8, 'MSES0': 9, 'MJES0': 10, 'FCJS0': 11, 'MCEM0': 12, 'FCAJ0': 13, 'FPMY0': 14, 'FKLC0': 15, 'FJSK0': 16, 'FNKL0': 17, 'FETB0': 18, 'FCLT0': 19, 'FLAS0': 20 , 'MJRF0': 21, 'MJDM0': 22, 'MDLD0': 23, 'MJDC0': 24}\n",
        "phonemes={'y': 0, 'n': 1, 'dx': 2, 'er': 3, 'eh': 4, 'el': 5, 'iy': 6, 'ow': 7, 'ah': 8, 'hv': 9, 'dcl': 10, 'g': 11, 'b': 12, 'p': 13, 'r': 14, 'm': 15, 'en': 16, 'z': 17, 'jh': 18, 'k': 19, 'ey': 20, 'kcl': 21, 'em': 22, 's': 23, 'ih': 24, 'axr': 25, 'd': 26, 'th': 27, 'f': 28, 'h#': 29, 'bcl': 30, 'ax-h': 31, 't': 32, 'ay': 33, 'w': 34, 'epi': 35, 'ux': 36, 'ix': 37, 'ae': 38, 'uh': 39, 'v': 40, 'ch': 41, 'hh': 42, 'ax': 43, 'tcl': 44, 'dh': 45, 'sh': 46, 'nx': 47, 'ao': 48, 'ng': 49, 'aw': 50, 'gcl': 51, 'l': 52, 'uw': 53, 'aa': 54, 'q': 55, 'eng': 56, 'oy': 57, 'pcl': 58, 'zh': 59, 'pau': 60}\n",
        "\n",
        "# reading wav file to get speech frames as a array\n",
        "def map_to_array(batch):\n",
        "  speech, _ = sf.read(batch[\"file\"])\n",
        "  batch[\"speech\"] = speech\n",
        "  return batch\n",
        "\n",
        "# getting train, test split of the dataset\n",
        "train_dataset=load_dataset(\"timit_asr\",split='train')\n",
        "test_dataset=load_dataset(\"timit_asr\",split='test')\n",
        "\n",
        "# array from wav file\n",
        "train_dataset = train_dataset.map(map_to_array)\n",
        "test_dataset = test_dataset.map(map_to_array)\n",
        "\n",
        "n=len(train_dataset)+len(test_dataset)\n",
        "permutation=np.random.permutation(n)\n",
        "\n",
        "# Opening files to pickle dump all attention layer representations for further classification\n",
        "input_file=open(\"x.txt\", \"wb\")\n",
        "target_file=open(\"y.txt\", \"wb\")\n",
        "print('total:',n)\n",
        "count=0\n",
        "y=[]\n",
        "\n",
        "# permutation for shuffling\n",
        "for i in permutation:\n",
        "  print('count',count)\n",
        "  count+=1\n",
        "  data=0\n",
        "  # mixing train and test data\n",
        "  if i<len(train_dataset):\n",
        "    data=train_dataset[int(i)]\n",
        "  else:\n",
        "    data=train_dataset[int(i)-len(train_dataset)]\n",
        "  \n",
        "  # taking only 25 speakers data\n",
        "  if data['speaker_id'] not in speakers:\n",
        "    continue\n",
        "\n",
        "  # preprocessing using Wav2Vec2Processor\n",
        "  x=processor(data[\"speech\"], return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "  phones=data['phonetic_detail']\n",
        "  # Generating data representations using Wav2Vec2Model\n",
        "  x=model.forward(x,output_hidden_states=True)\n",
        "  x=list(x.hidden_states)\n",
        "\n",
        "  # dividing the speech frames into groups corresponding to phone start and stop frame\n",
        "  for j in range(len(phones['start'])):\n",
        "    sampling=x[0].size(1)/len(data[\"speech\"])\n",
        "    start=math.floor(sampling*phones['start'][j])\n",
        "    stop=math.floor(sampling*phones['stop'][j])\n",
        "    phone=phones['utterance'][j]\n",
        "\n",
        "    temp=[]\n",
        "\n",
        "    # average pooling accross timeframes\n",
        "    for i, a in enumerate(x):\n",
        "      temp.append(torch.sum(a[0,start:stop,:],(0)))\n",
        "    # dumping all hidden activations\n",
        "    pickle.dump(temp, input_file)\n",
        "    # getting phone label\n",
        "    y.append(phonemes[phone])\n",
        "\n",
        "# dumping the class indices\n",
        "pickle.dump(y,target_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGP8WsKjamfc"
      },
      "source": [
        "# Loading the Speech Commands Dataset\n",
        "speech_commands=torchaudio.datasets.SPEECHCOMMANDS('/',download=True)\n",
        "\n",
        "# 35 Commands in the dataset\n",
        "commands_labels=['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
        "counts={'backward': 0, 'bed': 0, 'bird': 0, 'cat': 0, 'dog': 0, 'down': 0, 'eight': 0, 'five': 0, 'follow': 0, 'forward': 0, 'four': 0, 'go': 0, 'happy': 0, 'house': 0, 'learn': 0, 'left': 0, 'marvin': 0, 'nine': 0, 'no': 0, 'off': 0, 'on': 0, 'one': 0, 'right': 0, 'seven': 0, 'sheila': 0, 'six': 0, 'stop': 0, 'three': 0, 'tree': 0, 'two': 0, 'up': 0, 'visual': 0, 'wow': 0, 'yes': 0, 'zero': 0}\n",
        "class_ind={'backward': 0, 'bed': 1, 'bird': 2, 'cat': 3, 'dog': 4, 'down': 5, 'eight': 6, 'five': 7, 'follow': 8, 'forward': 9, 'four': 10, 'go': 11, 'happy': 12, 'house': 13, 'learn': 14, 'left': 15, 'marvin': 16, 'nine': 17, 'no': 18, 'off': 19, 'on': 20, 'one': 21, 'right': 22, 'seven': 23, 'sheila': 24, 'six': 25, 'stop': 26, 'three': 27, 'tree': 28, 'two': 29, 'up': 30, 'visual': 31, 'wow': 32, 'yes': 33, 'zero': 34}\n",
        "\n",
        "# Opening files to pickle dump all attention layer representations for further classification\n",
        "input_file=open(\"x.txt\", \"wb\")\n",
        "target_file=open(\"y.txt\", \"wb\")\n",
        "\n",
        "y=[]\n",
        "count=0\n",
        "# Getting hidden activations for all layers for each data instance\n",
        "for i, x in enumerate(speech_commands):\n",
        "  # 50 instance for each command taken\n",
        "  if counts[x[2]]>=50:\n",
        "    continue\n",
        "  counts[x[2]]=counts[x[2]]+1\n",
        "  \n",
        "  # preprocessing using Wav2Vec2Processor\n",
        "  input_values=processor(x[0],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "  # Generating data representations using Wav2Vec2Model\n",
        "  a=model.forward(input_values[0],output_hidden_states=True)\n",
        "  source=[]\n",
        "  # average pooling accross timeframes\n",
        "  for t in a.hidden_states:\n",
        "    source.append(torch.mean(t,(0,1)))\n",
        "  # dumping all hidden activations\n",
        "  pickle.dump(source,input_file)\n",
        "  y.append(class_ind[x[2]])\n",
        "  print(count)\n",
        "  count+=1\n",
        "# dumping the class indices\n",
        "pickle.dump(y,target_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}